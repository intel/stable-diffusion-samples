{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-to-Image Generation with Stable Diffusion and OpenVINOâ„¢\n",
    "\n",
    "Stable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/) and [LAION](https://laion.ai/). It is trained on 512x512 images from a subset of the [LAION-5B](https://laion.ai/blog/laion-5b/) database. This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder.\n",
    "See the [model card](https://huggingface.co/CompVis/stable-diffusion) for more information.\n",
    "\n",
    "General diffusion models are machine learning systems that are trained to denoise random gaussian noise step by step, to get to a sample of interest, such as an image.\n",
    "Diffusion models have shown to achieve state-of-the-art results for generating image data. But one downside of diffusion models is that the reverse denoising process is slow. In addition, these models consume a lot of memory because they operate in pixel space, which becomes unreasonably expensive when generating high-resolution images. Therefore, it is challenging to train these models and also use them for inference. OpenVINO brings capabilities to run model inference on Intel hardware and opens the door to the fantastic world of diffusion models for everyone!\n",
    "\n",
    "Model capabilities are not limited text-to-image only, it also is able solve additional tasks, for example text-guided image-to-image generation and inpainting. This tutorial also considers how to run text-guided image-to-image generation using Stable Diffusion.\n",
    "\n",
    "\n",
    "This notebook demonstrates how to convert and run stable diffusion model using OpenVINO.\n",
    "\n",
    "<img referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=5b5a4db0-7875-4bfb-bdbd-01698b5b1a77&file=notebooks/stable-diffusion-text-to-image/stable-diffusion-text-to-image.ipynb\" />\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table of contents:\n",
    "\n",
    "- [Prerequisites](#Prerequisites)\n",
    "- [Prepare Inference Pipelines](#Prepare-Inference-Pipelines)\n",
    "- [Text-to-image pipeline](#Text-to-image-pipeline)\n",
    "    - [Load Stable Diffusion model and create text-to-image pipeline](#Load-Stable-Diffusion-model-and-create-text-to-image-pipeline)\n",
    "    - [Text-to-Image generation](#Text-to-Image-generation)\n",
    "    - [Interactive text-to-image demo](#Interactive-text-to-image-demo)\n",
    "- [Image-to-Image pipeline](#Image-to-Image-pipeline)\n",
    "    - [Create image-to-Image pipeline](#Create-image-to-Image-pipeline)\n",
    "    - [Image-to-Image generation](#Image-to-Image-generation)\n",
    "    - [Interactive image-to-image demo](#Interactive-image-to-image-demo)\n",
    "\n",
    "### Installation Instructions\n",
    "\n",
    "This is a self-contained example that relies solely on its own code.\n",
    "\n",
    "We recommend  running the notebook in a virtual environment. You only need a Jupyter server to start.\n",
    "For details, please refer to [Installation Guide](https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/README.md#-installation-guide)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "[back to top â¬†ï¸](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "username = os.environ.get('USER')\n",
    "user_bin_path = os.path.expanduser(f\"/home/{username}/.local/bin\")\n",
    "sys.path.append(user_bin_path)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install -q \"openvino>=2023.1.0\" \"git+https://github.com/huggingface/optimum-intel.git\"\n",
    "!{sys.executable} -m pip install -q --extra-index-url https://download.pytorch.org/whl/cpu \"diffusers>=0.9.0\" \"torch>=2.1\"\n",
    "!{sys.executable} -m pip install -q \"huggingface-hub>=0.9.1\"\n",
    "!{sys.executable} -m pip install -q transformers Pillow opencv-python tqdm ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch `notebook_utils` module\n",
    "import requests\n",
    "\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    ")\n",
    "\n",
    "open(\"notebook_utils.py\", \"w\").write(r.text)\n",
    "\n",
    "from notebook_utils import download_file, device_widget"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Inference Pipelines\n",
    "[back to top â¬†ï¸](#Table-of-contents:)\n",
    "\n",
    "Let us now take a closer look at how the model works in inference by illustrating the logical flow."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sd-pipeline](https://user-images.githubusercontent.com/29454499/260981188-c112dd0a-5752-4515-adca-8b09bea5d14a.png)\n",
    "\n",
    "As you can see from the diagram, the only difference between Text-to-Image and text-guided Image-to-Image generation in approach is how initial latent state is generated. In case of Image-to-Image generation, you additionally have an image encoded by VAE encoder mixed with the noise produced by using latent seed, while in Text-to-Image you use only noise as initial latent state.\n",
    "The stable diffusion model takes both a latent image representation of size $64 \\times 64$ and a text prompt is transformed to text embeddings of size $77 \\times 768$ via CLIP's text encoder as an input.\n",
    "\n",
    "Next, the U-Net iteratively *denoises* the random latent image representations while being conditioned on the text embeddings. The output of the U-Net, being the noise residual, is used to compute a denoised latent image representation via a scheduler algorithm. Many different scheduler algorithms can be used for this computation, each having its pros and cons. For Stable Diffusion, it is recommended to use one of:\n",
    "\n",
    "- [PNDM scheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py)\n",
    "- [DDIM scheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py)\n",
    "- [K-LMS scheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py)(you will use it in your pipeline)\n",
    "\n",
    "Theory on how the scheduler algorithm function works is out of scope for this notebook. Nonetheless, in short, you should remember that you compute the predicted denoised image representation from the previous noise representation and the predicted noise residual.\n",
    "For more information, refer to the recommended [Elucidating the Design Space of Diffusion-Based Generative Models](https://arxiv.org/abs/2206.00364)\n",
    "\n",
    "The *denoising* process is repeated given number of times (by default 50) to step-by-step retrieve better latent image representations.\n",
    "When complete, the latent image representation is decoded by the decoder part of the variational auto encoder."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-to-image pipeline\n",
    "[back to top â¬†ï¸](#Table-of-contents:)\n",
    "\n",
    "\n",
    "### Load Stable Diffusion model and create text-to-image pipeline\n",
    "[back to top â¬†ï¸](#Table-of-contents:)\n",
    "\n",
    "We will load optimized Stable Diffusion model from the Hugging Face Hub and create pipeline to run an inference with OpenVINO Runtime by [Optimum Intel](https://huggingface.co/docs/optimum/intel/inference#stable-diffusion). \n",
    "\n",
    "For running the Stable Diffusion model with Optimum Intel, we will use the `optimum.intel.OVStableDiffusionPipeline` class, which represents the inference pipeline. `OVStableDiffusionPipeline` initialized by the `from_pretrained` method. It supports on-the-fly conversion models from PyTorch using the `export=True` parameter. A converted model can be saved on disk using the `save_pretrained` method for the next running.\n",
    "\n",
    "When Stable Diffusion models are exported to the OpenVINO format, they are decomposed into three components that consist of four models combined during inference into the pipeline:\n",
    "\n",
    "* The text encoder\n",
    "    * The text-encoder is responsible for transforming the input prompt(for example \"a photo of an astronaut riding a horse\") into an embedding space that can be understood by the U-Net. It is usually a simple transformer-based encoder that maps a sequence of input tokens to a sequence of latent text embeddings.\n",
    "* The U-NET\n",
    "    * Model predicts the `sample` state for the next step.\n",
    "* The VAE encoder\n",
    "    * The encoder is used to convert the image into a low dimensional latent representation, which will serve as the input to the U-Net model.\n",
    "* The VAE decoder\n",
    "    * The decoder transforms the latent representation back into an image."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select device from dropdown list for running inference using OpenVINO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = device_widget()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.intel.openvino import OVStableDiffusionPipeline\n",
    "from pathlib import Path\n",
    "\n",
    "DEVICE = device.value\n",
    "\n",
    "MODEL_ID = \"prompthero/openjourney\"\n",
    "MODEL_DIR = Path(\"diffusion_pipeline\")\n",
    "\n",
    "if not MODEL_DIR.exists():\n",
    "    ov_pipe = OVStableDiffusionPipeline.from_pretrained(MODEL_ID, export=True, device=DEVICE, compile=False)\n",
    "    ov_pipe.save_pretrained(MODEL_DIR)\n",
    "else:\n",
    "    ov_pipe = OVStableDiffusionPipeline.from_pretrained(MODEL_DIR, device=DEVICE, compile=False)\n",
    "\n",
    "ov_pipe.compile()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-to-Image generation\n",
    "[back to top â¬†ï¸](#Table-of-contents:)\n",
    "\n",
    "Now, you can define a text prompt for image generation and run inference pipeline.\n",
    "\n",
    "> **Note**: Consider increasing `steps` to get more precise results. A suggested value is `50`, but it will take longer time to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "sample_text = (\n",
    "    \"cyberpunk cityscape like Tokyo New York  with tall buildings at dusk golden hour cinematic lighting, epic composition. \"\n",
    "    \"A golden daylight, hyper-realistic environment. \"\n",
    "    \"Hyper and intricate detail, photo-realistic. \"\n",
    "    \"Cinematic and volumetric light. \"\n",
    "    \"Epic concept art. \"\n",
    "    \"Octane render and Unreal Engine, trending on artstation\"\n",
    ")\n",
    "text_prompt = widgets.Text(value=sample_text, description=\"your text\")\n",
    "num_steps = widgets.IntSlider(min=1, max=50, value=20, description=\"steps:\")\n",
    "seed = widgets.IntSlider(min=0, max=10000000, description=\"seed: \", value=42)\n",
    "widgets.VBox([text_prompt, num_steps, seed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pipeline settings\")\n",
    "print(f\"Input text: {text_prompt.value}\")\n",
    "print(f\"Seed: {seed.value}\")\n",
    "print(f\"Number of steps: {num_steps.value}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate an image and save the generation results.\n",
    "The pipeline returns one or several results: `images` contains final generated image. To get more than one result, you can set the `num_images_per_prompt` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(seed.value)\n",
    "\n",
    "result = ov_pipe(text_prompt.value, num_inference_steps=num_steps.value)\n",
    "\n",
    "final_image = result[\"images\"][0]\n",
    "final_image.save(\"result.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is show time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\\n\\t\".join(text_prompt.value.split(\".\"))\n",
    "print(\"Input text:\")\n",
    "print(\"\\t\" + text)\n",
    "display(final_image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the image was rendered in high definition ðŸ”¥."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PyTorch GPU",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "openvino_notebooks": {
   "imageUrl": "https://user-images.githubusercontent.com/29454499/216524089-ed671fc7-a78b-42bf-aa96-9f7c791a9419.png",
   "tags": {
    "categories": [
     "Model Demos"
    ],
    "libraries": [],
    "other": [
     "Stable Diffusion"
    ],
    "tasks": [
     "Text-to-Image"
    ]
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "30f6166f5f0cb6253cad15b1c8ca46093b160f1914c051aeccf8063f98b299b9"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
