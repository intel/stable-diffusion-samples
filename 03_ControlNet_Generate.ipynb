{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4158cca8",
   "metadata": {},
   "source": [
    "# ControlNet Image Generation with Stable Diffusion\n",
    "\n",
    "This notebook demonstrates how to use ControlNet with Stable Diffusion to generate images guided by edge detection. ControlNet allows us to condition the image generation process on structural information from an input image while maintaining the freedom to change the style and content.\n",
    "\n",
    "In this example, we'll:\n",
    "1. Load and prepare an input image\n",
    "2. Apply Canny edge detection to extract structural information\n",
    "3. Use Intel optimization with IPEX to accelerate inference\n",
    "4. Generate a new image based on the edge map and a text prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2551519",
   "metadata": {},
   "source": [
    "## Import Base Libraries\n",
    "\n",
    "First, we import the core libraries needed for deep learning:\n",
    "- `torch`: PyTorch for deep learning operations\n",
    "- `nn`: Neural network modules from PyTorch\n",
    "- `intel_extension_for_pytorch` (IPEX): Intel's extension to optimize PyTorch performance on Intel hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f2f527-6dce-411e-bc86-6128890e8315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import intel_extension_for_pytorch as ipex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf82120b",
   "metadata": {},
   "source": [
    "## Import Image Processing and Diffusion Libraries\n",
    "\n",
    "Next, we import libraries for image processing and the diffusion model:\n",
    "- `diffusers`: HuggingFace's library for diffusion models including Stable Diffusion\n",
    "- `PIL`: Python Imaging Library for image manipulation\n",
    "- `cv2`: OpenCV for computer vision tasks, particularly edge detection\n",
    "- `numpy`: For numerical operations on image arrays\n",
    "- We also import specific components needed for the ControlNet pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37881584-df16-4845-9875-2fdfbbbba9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils import load_image, make_image_grid\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f427a658",
   "metadata": {},
   "source": [
    "## Load Image, Apply Edge Detection, and Generate with ControlNet\n",
    "\n",
    "This section contains the main workflow:\n",
    "\n",
    "1. **Load and Resize Input Image**: We load a house outline image and resize it to 1024x1024\n",
    "\n",
    "2. **Define Pipeline Optimization Function**: The `optimize_pipeline` function leverages Intel IPEX to accelerate model inference\n",
    "\n",
    "3. **Apply Canny Edge Detection**: We extract edges from the original image to guide the generation\n",
    "   - Low threshold (100) and high threshold (200) control the edge sensitivity\n",
    "   - The resulting edge map is converted to an RGB image\n",
    "\n",
    "4. **Set Up the ControlNet Model**:\n",
    "   - We load a pre-trained ControlNet model specialized for Canny edge maps\n",
    "   - We also load the base Stable Diffusion v1.5 model\n",
    "   - Models are configured to use bfloat16 precision for efficiency\n",
    "\n",
    "5. **Generate the Image**:\n",
    "   - We run inference with the prompt \"Batman\" guided by our edge map\n",
    "   - Intel acceleration with XPU optimizations is employed\n",
    "\n",
    "6. **Display and Save the Results**:\n",
    "   - We create a grid showing the original image, edge map, and generated result\n",
    "   - The output is saved as \"house.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce5ca9d-dee8-4932-87a9-85da22ea76da",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image = load_image(\n",
    "     \"https://img.freepik.com/premium-photo/house-outline-illustration-white-background_1112329-31710.jpg\"\n",
    ").resize((1024, 1024))\n",
    "\n",
    "def optimize_pipeline(pipeline):\n",
    "    \"\"\"\n",
    "    Optimizes the model for inference using ipex.\n",
    "\n",
    "    Parameters:\n",
    "    - pipeline: The model pipeline to be optimized.\n",
    "\n",
    "    Returns:\n",
    "    - pipeline: The optimized model pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    for attr in dir(pipeline):\n",
    "        try:\n",
    "            if isinstance(getattr(pipeline, attr), nn.Module):\n",
    "                setattr(\n",
    "                    pipeline,\n",
    "                    attr,\n",
    "                    ipex.optimize(\n",
    "                        getattr(pipeline, attr).eval(),\n",
    "                        dtype=pipeline.text_encoder.dtype,\n",
    "                        inplace=True,\n",
    "                    ),\n",
    "                )\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    return pipeline\n",
    "\n",
    "image = np.array(original_image)\n",
    "\n",
    "low_threshold = 100\n",
    "high_threshold = 200\n",
    "\n",
    "image = cv2.Canny(image, low_threshold, high_threshold)\n",
    "image = image[:, :, None]\n",
    "image = np.concatenate([image, image, image], axis=2)\n",
    "canny_image = Image.fromarray(image)\n",
    "\n",
    "image_tensor = torch.tensor(np.array(canny_image)).to(\"xpu\")\n",
    "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.bfloat16, use_safetensors=True)\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.bfloat16, use_safetensors=True\n",
    ")\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe = pipe.to(\"xpu\")\n",
    "pipe = optimize_pipeline(pipe)\n",
    "# pipe.enable_xformers_memory_efficient_attention()\n",
    "with torch.inference_mode():\n",
    "    with torch.xpu.amp.autocast(enabled=True, dtype=torch.bfloat16):\n",
    "        output = pipe(\n",
    "            \"Vivid Watercolor\", image=canny_image\n",
    "        ).images[0]\n",
    "\n",
    "image_grid = make_image_grid([original_image, canny_image, output], rows=1, cols=3)\n",
    "#image_grid = image_grid.to(\"xpu\")\n",
    "image_grid_np = np.array(image_grid)\n",
    "cv2.imwrite(\"house.png\", image_grid_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff0deff",
   "metadata": {},
   "source": [
    "## Results and Next Steps\n",
    "\n",
    "The generated image combines the structural information from the house outline with the style and visual elements associated with a vivid watercolor interpretation. The output is saved as \"house.png\" with three panels showing:\n",
    "- Left: The original house outline image\n",
    "- Middle: The Canny edge detection result\n",
    "- Right: The generated image combining the structure with the \"Vivid Watercolor\" prompt\n",
    "\n",
    "To experiment further:\n",
    "- Try different edge detection thresholds to see how they affect the results\n",
    "- Use different text prompts to generate various styles while keeping the same structure\n",
    "- Experiment with other ControlNet models like depth maps, segmentation maps, or pose estimation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdxlturbo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
